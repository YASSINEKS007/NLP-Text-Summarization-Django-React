import os
import re
from textwrap import dedent

import google.generativeai as genai
import networkx as nx
import json
import numpy as np
import pandas as pd
import PyPDF2
from docx import Document
from dotenv import load_dotenv
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize
from rest_framework import status
from rest_framework.decorators import (api_view, authentication_classes,
                                       parser_classes, permission_classes)
from rest_framework.parsers import FormParser, MultiPartParser
from rest_framework.permissions import AllowAny, IsAuthenticated
from rest_framework.response import Response
from rest_framework_simplejwt.authentication import JWTAuthentication
from sklearn.metrics.pairwise import cosine_similarity

from .models import Summary
from .serializers import SummarySerializer

# Ensure nltk stopwords are downloaded
# nltk.download('stopwords')

# Load word embeddings


def load_word_embeddings(file_path=os.path.join(os.path.dirname(os.path.abspath(__file__)), './glove.6B.100d.txt')):
    word_embeddings = {}
    with open(file_path, encoding='utf-8') as f:
        for line in f:
            values = line.split()
            word = values[0]
            coefs = np.asarray(values[1:], dtype='float32')
            word_embeddings[word] = coefs
    return word_embeddings


def remove_stopwords(sentence, stop_words):
    return " ".join([word for word in sentence.split() if word not in stop_words])

# Clean the summary text


def clean_summary(text):
    # Replace \x93 and \x94 with "
    cleaned_text = re.sub(r'\\x9[3|4]', '"', text)
    cleaned_text = re.sub(r'\\x92', "'", cleaned_text)  # Replace \x92 with '
    cleaned_text = re.sub(r'[\x93]', '"', cleaned_text)  # Left double quotes
    cleaned_text = re.sub(r'[\x94]', '"', cleaned_text)  # Right double quotes
    cleaned_text = re.sub(r'[\x92]', "'", cleaned_text)  # Right single quote
    # Replace multiple spaces with a single space
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
    return cleaned_text.strip()


def get_gemini_response(input_text):
    try:
        model = genai.GenerativeModel('gemini-pro')
        response = model.generate_content([input_text])
        return response.text
    except Exception as e:
        print("Error connecting to Gemini AI:", e)


load_dotenv()
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
word_embeddings = load_word_embeddings()


def generate_summary(text: str, summary_len: int):
    sentences = sent_tokenize(text)
    clean_sentences = pd.Series(sentences).str.replace(
        "[^a-zA-Z ]", " ", regex=True)
    clean_sentences = [s.lower().strip() for s in clean_sentences]

    stop_words = set(stopwords.words('english'))
    clean_sentences = [remove_stopwords(
        sentence, stop_words) for sentence in clean_sentences]

    # Sentence vectorization
    sentence_vectors = []
    for sentence in clean_sentences:
        if len(sentence) > 0:
            v = sum([word_embeddings.get(word, np.zeros((100,)))
                    for word in sentence.split()]) / (len(sentence.split()) + 0.001)
        else:
            v = np.zeros((100,))
        sentence_vectors.append(v)

    # Cosine similarity
    sim_mat = np.zeros([len(sentences), len(sentences)])
    for i in range(len(sentences)):
        for j in range(len(sentences)):
            if i != j:
                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(
                    1, -1), sentence_vectors[j].reshape(1, -1))[0, 0]

    # Pagerank
    nx_graph = nx.from_numpy_array(sim_mat)
    scores = nx.pagerank(nx_graph)
    ranked_sentences = sorted(
        ((scores[i], s) for i, s in enumerate(sentences)), reverse=True)

    # Generate and clean summary
    summary = " ".join([ranked_sentences[i][1]
                       for i in range(min(10, len(ranked_sentences)))])
    cleaned_summary = clean_summary(summary)

    enhanced_summary = get_gemini_response(dedent(f"""
        Improve the summary generated by the TextRank algorithm by performing the following steps:
        1. Expand on the key points, providing detailed explanations for each one.
        2. Add context and examples where necessary to ensure the summary is easily understandable.
        3. Ensure the summary is written in clear, engaging, and accessible language, avoiding jargon.
        4. Limit the final version to {summary_len} sentences.
        5. Return the result as a valid JSON object with two fields:
            {{
                "title": "Your Summary Title",
                "summary": "Your Improved Summary Text"
            }}

        Original summary: {cleaned_summary}
    """))

    # Remove the Markdown code block formatting
    json_str = enhanced_summary.strip('```json\n').strip('```')

    # Parse the JSON string
    json_data = json.loads(json_str)

    return json_data


@api_view(["POST"])
@permission_classes([IsAuthenticated])
@authentication_classes([JWTAuthentication])
def generate_summary_text(request):
    text_to_summarize = request.data.get("text_to_summarize")
    summary_len = request.data.get('summary_len')
    summary = generate_summary(text_to_summarize, summary_len=summary_len)

    user = request.user

    Summary.objects.create(user=user, summary_text=summary["summary"], summary_title=summary['title'],
                           text_source=text_to_summarize)

    return Response({"summary": summary}, status=status.HTTP_201_CREATED)


@api_view(["POST"])
@permission_classes([IsAuthenticated])
@authentication_classes([JWTAuthentication])
@parser_classes([MultiPartParser, FormParser])
def generate_summary_document(request):
    file = request.FILES.get("file")
    summary_len = request.data.get("summary_len")

    if not file:
        return Response({"error": "No file provided"}, status=status.HTTP_400_BAD_REQUEST)

    try:
        extracted_text = ""

        # Determine file type and extract text
        if file.name.endswith(".pdf"):
            pdf_reader = PyPDF2.PdfReader(file)
            for page in pdf_reader.pages:
                extracted_text += page.extract_text()

        elif file.name.endswith(".docx"):
            doc = Document(file)
            for paragraph in doc.paragraphs:
                extracted_text += paragraph.text + "\n"

        else:
            return Response({"error": "Unsupported file type. Only PDF and DOCX are supported."},
                            status=status.HTTP_400_BAD_REQUEST)

        # Generate the summary
        summary = generate_summary(extracted_text, summary_len=summary_len)
        print(summary)

        # Save the summary to the database
        user = request.user

        Summary.objects.create(
            user=user, summary_text=summary["summary"], summary_title=summary['title'], pdf_or_word_file=f"File: {file.name}")
        

        return Response({"summary": summary}, status=status.HTTP_201_CREATED)

    except Exception as e:
        return Response({"error": f"Failed to process the file: {str(e)}"},
                        status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(["GET"])
@permission_classes([IsAuthenticated])
@authentication_classes([JWTAuthentication])
def get_summaries(request):
    summaries = Summary.objects.all()
    serializer = SummarySerializer(summaries, many=True)
    return Response({"summaries": serializer.data}, status=status.HTTP_200_OK)



@api_view(["DELETE"])
@permission_classes([IsAuthenticated])
@authentication_classes([JWTAuthentication])
def delete_summary(request, id):
    try:
        summary = Summary.objects.get(id=id)
    except Summary.DoesNotExist:
        return Response({"message": "Summary not found"}, status=status.HTTP_404_NOT_FOUND)
    
    summary.delete()
    
    return Response({"message": "Summary deleted successfully"}, status=status.HTTP_200_OK)
